{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a080151f",
   "metadata": {},
   "source": [
    "# Connecting to PostgreSQL from Python\n",
    "\n",
    "In this notebook, we will learn how to connect a PostgreSQL database to Python.  \n",
    "This is an essential skill if you want to:\n",
    "- Explore your data directly in **pandas** DataFrames  \n",
    "- Build **data pipelines (ETL/ELT)** to refresh your tables regularly  \n",
    "- Power **machine learning or LLM-based applications** that rely on structured data  \n",
    "- Prototype **dashboards and APIs** that serve insights to end-users  \n",
    "\n",
    "We will go step by step:\n",
    "1. Load database credentials securely from a `.env` file  \n",
    "2. Connect to PostgreSQL using **SQLAlchemy** and **psycopg2**  \n",
    "3. Run sanity checks to confirm the connection  \n",
    "4. Query our nightlights and population tables into pandas  \n",
    "5. Perform simple summaries, joins, and a time-series query  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d556cd",
   "metadata": {},
   "source": [
    "## Required Python Packages\n",
    "Ensure that you installed the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491440a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ecdd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/risa/anaconda3/lib/python3.13/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef232a12",
   "metadata": {},
   "source": [
    "## PostgreSQL Environment Variables\n",
    "To connect, PostgreSQL needs a few basic pieces of information:\n",
    "\n",
    "- **PGHOST** → The host where PostgreSQL is running.  \n",
    "  For local setups this is usually `localhost`.  \n",
    "- **PGPORT** → The port PostgreSQL listens on.  \n",
    "  Default is `5432`.  \n",
    "- **PGDATABASE** → The database name you created for this lab, e.g. `ntl_pop`.  \n",
    "- **PGUSER** → Your PostgreSQL username.  \n",
    "  This is usually the same as your system username when running locally.  \n",
    "\n",
    "⚠️ **Note:** On a local setup, you usually don’t need a password if PostgreSQL is configured to trust local connections.  \n",
    "In that case, we will not use `PGPASSWORD`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f789d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load environment variables\n",
    "# The .env file should be in the repo root with keys:\n",
    "# PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD\n",
    "\n",
    "# looks for .env in current directory by default\n",
    "load_dotenv()  \n",
    "\n",
    "PGHOST = os.getenv(\"PGHOST\")\n",
    "PGPORT = os.getenv(\"PGPORT\", \"5432\")\n",
    "PGDATABASE = os.getenv(\"PGDATABASE\")\n",
    "PGUSER = os.getenv(\"PGUSER\")\n",
    "PGPASSWORD = os.getenv(\"PGPASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e79cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGHOST= 'localhost'\n",
    "PGPORT=5432\n",
    "PGDATABASE= 'ntl_pop'\n",
    "PGUSER= 'risa'\n",
    "PGPASSWORD=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d11490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL Connection Settings:\n",
      "Host: localhost\n",
      "Port: 5432\n",
      "Database: ntl_pop\n",
      "User: risa\n",
      "Password: [NOT SET]\n"
     ]
    }
   ],
   "source": [
    "print(\"PostgreSQL Connection Settings:\")\n",
    "print(f\"Host: {PGHOST}\")\n",
    "print(f\"Port: {PGPORT}\")\n",
    "print(f\"Database: {PGDATABASE}\")\n",
    "print(f\"User: {PGUSER}\")\n",
    "print(f\"Password: {'[SET]' if PGPASSWORD else '[NOT SET]'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bbf71",
   "metadata": {},
   "source": [
    "## Create  SQLAlchemy engine \n",
    "At this stage, we are establishing the **connection between Python and PostgreSQL**.  \n",
    "The `SQLAlchemy` package plays the role of a **database toolkit and Object Relational Mapper (ORM)**. In our case, we are mainly using it as a **bridge**: it translates Python code into SQL statements that PostgreSQL can understand, and it manages the underlying connection details for us (user, host, port, database). By creating an **engine object**, we set up a reusable gateway that allows us to open sessions, run queries, and easily pull results into Python tools such as **pandas** for analysis. While `psycopg2` handles the low-level communication with PostgreSQL, `SQLAlchemy` provides a higher-level, more user-friendly interface.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1eecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection psql string: postgresql+psycopg2://risa:@localhost:5432/ntl_pop\n"
     ]
    }
   ],
   "source": [
    "connection_string = f\"postgresql+psycopg2://{PGUSER}:{PGPASSWORD}@{PGHOST}:{PGPORT}/{PGDATABASE}\"\n",
    "print(\"Connection psql string:\", connection_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef2d282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as: ('risa', 'ntl_pop')\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\n",
    "    connection_string,\n",
    "    pool_pre_ping=True,\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "with engine.connect() as conn:\n",
    "    who = conn.execute(text(\"SELECT current_user, current_database();\")).fetchone()\n",
    "    print(\"Connected as:\", who)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef873a3",
   "metadata": {},
   "source": [
    "## Running SQL Commands\n",
    "Once the connection is established through the SQLAlchemy engine, we can begin sending SQL commands from Python.  \n",
    "What happens in the background is:  \n",
    "\n",
    "1. **You write a SQL query as a string in Python** (e.g., `\"SELECT * FROM ntl_annual LIMIT 5;\"`).  \n",
    "2. **SQLAlchemy passes this query to the underlying driver (`psycopg2`)**, which handles the low-level communication with PostgreSQL.  \n",
    "3. **PostgreSQL executes the query on the database** and returns the results (rows and columns).  \n",
    "4. **SQLAlchemy collects the results** and makes them available to Python.  \n",
    "5. If we use `pandas.read_sql`, the results are automatically converted into a **DataFrame** for analysis, filtering, and visualization.  \n",
    "\n",
    "In short: Python → SQLAlchemy → psycopg2 → PostgreSQL → back to Python as DataFrame.  \n",
    "This allows us to stay in a familiar Python environment while still harnessing the power of SQL.  \n",
    "\n",
    "In the same way, we can also **create new tables**, **update existing tables with additional data**, or even **delete records**—all from within Python.  \n",
    "For example:  \n",
    "- Use `CREATE TABLE` statements to define new tables.  \n",
    "- Use `INSERT` or `COPY` to add more rows of data.  \n",
    "- Use `UPDATE` to modify existing records.  \n",
    "- Use `DROP` to remove tables you no longer need.  \n",
    "\n",
    "This makes Python a powerful interface for both **querying** and **managing** your database directly.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f3cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(sql: str) -> pd.DataFrame:\n",
    "    \"\"\"Run a SQL query and return the results as a pandas DataFrame.\"\"\"\n",
    "    return pd.read_sql(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96cffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pandas.io.sql.read_sql(sql, con, index_col: 'str | list[str] | None' = None, coerce_float: 'bool' = True, params=None, parse_dates=None, columns: 'list[str] | None' = None, chunksize: 'int | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, dtype: 'DtypeArg | None' = None) -> 'DataFrame | Iterator[DataFrame]'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9b26b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Table counts ---\n",
      "cells: 2169 rows\n",
      "pop: 2169 rows\n",
      "ntl_annual: 52056 rows\n",
      "ntl_monthly: 650700 rows\n",
      "\n",
      "--- First 5 rows from cells ---\n",
      "         cell_id province_name district_name sector_name cell_name\n",
      "0  RWA.1.1.1.1_1  Amajyaruguru        Burera      Bungwe    Bungwe\n",
      "1  RWA.1.1.1.2_1  Amajyaruguru        Burera      Bungwe  Bushenya\n",
      "2  RWA.1.1.1.3_1  Amajyaruguru        Burera      Bungwe  Mudugari\n",
      "3  RWA.1.1.1.4_1  Amajyaruguru        Burera      Bungwe     Tumba\n",
      "4  RWA.1.1.2.1_1  Amajyaruguru        Burera      Butaro   Gatsibo\n"
     ]
    }
   ],
   "source": [
    "# ========== 3. Sanity checks ==========\n",
    "tables = [\"cells\", \"pop\", \"ntl_annual\", \"ntl_monthly\"]\n",
    "\n",
    "print(\"\\n--- Table counts ---\")\n",
    "for t in tables:\n",
    "    cnt = peek(f\"SELECT COUNT(*) AS n FROM {t};\")\n",
    "    print(f\"{t}: {cnt.loc[0, 'n']} rows\")\n",
    "\n",
    "print(\"\\n--- First 5 rows from cells ---\")\n",
    "print(peek(\"SELECT * FROM cells LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08523890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__firstlineno__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_compiled_cache',\n",
       " '_connection_cls',\n",
       " '_echo',\n",
       " '_execution_options',\n",
       " '_has_events',\n",
       " '_is_future',\n",
       " '_lru_size_alert',\n",
       " '_option_cls',\n",
       " '_optional_conn_ctx_manager',\n",
       " '_run_ddl_visitor',\n",
       " '_schema_translate_map',\n",
       " '_should_log_debug',\n",
       " '_should_log_info',\n",
       " '_sqla_logger_namespace',\n",
       " 'begin',\n",
       " 'clear_compiled_cache',\n",
       " 'connect',\n",
       " 'dialect',\n",
       " 'dispatch',\n",
       " 'dispose',\n",
       " 'driver',\n",
       " 'echo',\n",
       " 'engine',\n",
       " 'execution_options',\n",
       " 'get_execution_options',\n",
       " 'hide_parameters',\n",
       " 'logger',\n",
       " 'logging_name',\n",
       " 'name',\n",
       " 'pool',\n",
       " 'raw_connection',\n",
       " 'update_execution_options',\n",
       " 'url']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "385c2f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Average ntl_mean per year ---\n",
      "   year  avg_ntl_mean\n",
      "0  2012      0.207097\n",
      "1  2013      0.253869\n",
      "2  2014      0.257401\n",
      "3  2015      0.244143\n",
      "4  2016      0.257303\n",
      "\n",
      "--- Top 5 brightest cells in 2023 ---\n",
      "          cell_id ntl_mean\n",
      "0   RWA.5.3.6.6_1     None\n",
      "1   RWA.5.3.1.3_1     None\n",
      "2   RWA.5.3.1.2_1     None\n",
      "3  RWA.2.2.15.1_1     None\n",
      "4   RWA.5.3.7.2_1     None\n",
      "\n",
      "--- Light per capita (2023) ---\n",
      "  cell_name district_name  year  light_per_capita  ntl_sum  general_pop\n",
      "0   LacKivu        Rubavu  2023          0.513305    119.6        233.0\n",
      "1   LacKivu       Rutsiro  2023          0.252273     22.2         88.0\n",
      "2   LacKivu       Karongi  2023          0.143460     34.0        237.0\n",
      "3   Biryogo    Nyarugenge  2023          0.111834     94.5        845.0\n",
      "4    Kiyovu    Nyarugenge  2023          0.071732    894.0      12463.0\n",
      "5   LacKivu        Rusizi  2023          0.071368     16.7        234.0\n",
      "6    Karera      Bugesera  2023          0.068717     64.8        943.0\n",
      "7   LacKivu    Nyamasheke  2023          0.066667     12.4        186.0\n",
      "8  Kamukina        Gasabo  2023          0.060964    275.8       4524.0\n",
      "9   Rugando        Gasabo  2023          0.051776    428.6       8278.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== 4. Annual summaries ==========\n",
    "print(\"\\n--- Average ntl_mean per year ---\")\n",
    "annual_avg = peek(\"\"\"\n",
    "SELECT year, AVG(ntl_mean) AS avg_ntl_mean\n",
    "FROM ntl_annual\n",
    "GROUP BY year\n",
    "ORDER BY year;\n",
    "\"\"\")\n",
    "print(annual_avg.head())\n",
    "\n",
    "print(\"\\n--- Top 5 brightest cells in 2023 ---\")\n",
    "brightest = peek(\"\"\"\n",
    "SELECT cell_id, ntl_mean\n",
    "FROM ntl_annual\n",
    "WHERE year = 2023\n",
    "ORDER BY ntl_mean DESC\n",
    "LIMIT 5;\n",
    "\"\"\")\n",
    "print(brightest)\n",
    "\n",
    "\n",
    "# ========== 5. Join with population ==========\n",
    "print(\"\\n--- Light per capita (2023) ---\")\n",
    "per_capita = peek(\"\"\"\n",
    "SELECT\n",
    "  c.cell_name,\n",
    "  c.district_name,\n",
    "  a.year,\n",
    "  a.ntl_sum / NULLIF(p.general_pop, 0) AS light_per_capita,\n",
    "  a.ntl_sum,\n",
    "  p.general_pop\n",
    "FROM ntl_annual a\n",
    "JOIN cells c ON a.cell_id = c.cell_id\n",
    "JOIN pop   p ON a.cell_id = p.cell_id\n",
    "WHERE a.year = 2023\n",
    "ORDER BY light_per_capita DESC NULLS LAST\n",
    "LIMIT 10;\n",
    "\"\"\")\n",
    "print(per_capita)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0dbfd5",
   "metadata": {},
   "source": [
    "# Exercise: Working with Your Database\n",
    "\n",
    "Now that you’ve seen how to connect and query PostgreSQL from Python, it’s time to practice.  \n",
    "In this exercise, you will write and run your own SQL commands inside Python.  \n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Create a New Table**  \n",
    "   - Write a SQL command in Python to create a small table called `demo_cells` with the following columns:  \n",
    "     - `id` (integer, primary key)  \n",
    "     - `cell_name` (text)  \n",
    "     - `population` (integer)  \n",
    "\n",
    "2. **Insert Data**  \n",
    "   - Insert at least **two rows** of data into `demo_cells`.  \n",
    "   - Hint: use `INSERT INTO demo_cells (...) VALUES (...);`.  \n",
    "\n",
    "3. **Query Your Data**  \n",
    "   - Select all rows from `demo_cells` and load them into a pandas DataFrame.  \n",
    "   - Display the results.  \n",
    "\n",
    "4. **Update Data**  \n",
    "   - Update one of the rows (for example, change the population of a cell).  \n",
    "   - Run a `SELECT` again to confirm the update worked.  \n",
    "\n",
    "5. **Cleanup (Optional)**  \n",
    "   - Drop the table when you are done: `DROP TABLE demo_cells;`.  \n",
    "\n",
    "---\n",
    "\n",
    "💡 *Tip:* Remember that you can use the `engine.execute(text(\"SQL HERE\"))` pattern for commands like `CREATE`, `INSERT`, `UPDATE`, or `DROP`, and `pd.read_sql(\"SQL HERE\", engine)` when you want to return results into a DataFrame.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edcf84",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Summary: Working with PostgreSQL in Python\n",
    "\n",
    "In this notebook, you learned how to connect Python to a PostgreSQL database, configure the connection using environment variables, and create a SQLAlchemy engine that acts as a bridge between Python and SQL. You executed key operations such as checking table counts, exploring data with a helper function, and running queries to analyze nightlight trends, identify the brightest cells, and compute light-per-capita statistics by joining nightlights with population data. In the same way, Python can interact with many other databases — whether they are hosted locally on your machine, running on a server in the cloud, or provided as enterprise solutions. This includes both proprietary systems such as Microsoft SQL Server and Oracle, as well as open-source databases like MySQL, MariaDB, and SQLite. Thanks to libraries like SQLAlchemy, the workflow you practiced here is portable: once you learn how to query and manage data in Python, you can apply the same approach across different database platforms.\n",
    "\n",
    "## Next Steps: Exploring Databases with Python\n",
    "After learning how to connect Python to PostgreSQL and run queries, you can explore:  \n",
    "\n",
    "- **Advanced SQL**: joins, window functions, subqueries, CTEs  \n",
    "- **Database management**: creating/updating tables, indexes, transactions  \n",
    "- **Python integration**: automate queries, build ETL pipelines, visualize results  \n",
    "- **Other databases**: SQLite, MySQL/MariaDB, SQL Server, Oracle  \n",
    "- **Scaling up**: cloud-hosted databases, connection pooling, performance tuning  \n",
    "- **Applications**: power ML workflows, LLM-based apps (RAG), dashboards, or APIs  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef87e9a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
